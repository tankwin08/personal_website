{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Tan Zhou","text":""},{"location":"#about-me","title":"About Me","text":"<p>Hello! I'm Tan Zhou, a Data Scientist and ML professional with over 8 years of experience in machine learning, Generative AI, and statistical modeling. Currently, I lead data science initiatives, where I developed innovative GenAI solutions\u2014like an enterprise HR bot and a document processor\u2014to drive efficiency and solve complex business challenges.</p> <p>With a Ph.D. in Applied Statistics &amp; Geocomputation from Texas A&amp;M University, I specialize in leveraging ML/DL, LLMs, and advanced analytics to transform data into actionable insights and drive business value and impact. My work spans industries, from Finanace (e.g. pricing systems, contract health), HR, and Operation (predictive maintenance) to geospatial analysis and sustainability dashboards. I'm passionate about building scalable AI solutions and collaborating with teams to deliver impact.</p>"},{"location":"#contact-location","title":"Contact &amp; Location","text":"<ul> <li>Location: St. Louis, MO</li> <li>Email: tankchow12@gmail.com</li> </ul>"},{"location":"about/","title":"About Me","text":""},{"location":"about/#professional-background","title":"Professional Background","text":"<p>I am a Senior Data Scientist with 8+ years of experience in both industry and academia. My work focuses on developing innovative solutions using quantitative and machine learning models for complex business problems.</p>"},{"location":"about/#areas-of-expertise","title":"Areas of Expertise","text":"<ul> <li>Machine Learning &amp; AI</li> <li>Statistical Analysis</li> <li>Time Series Analysis</li> <li>Natural Language Processing</li> <li>Computer Vision</li> <li>Remote Sensing</li> <li>Big Data Processing</li> <li>Geospatial Data Engineering</li> </ul>"},{"location":"about/#academic-background","title":"Academic Background","text":"<ul> <li>Ph.D. in Applied Statistics and Geo-computation from Texas A&amp;M University</li> <li>Focus on signal processing, Bayesian methods, and Machine learning</li> <li>Research expertise in LiDAR data processing and analysis</li> </ul>"},{"location":"about/#current-work-focus","title":"Current Work Focus","text":"<ul> <li>Developing machine learning solutions for business optimization</li> <li>Algorithm development and implementation</li> <li>Processing and analyzing various data types:</li> <li>Time series data</li> <li>Text and NLP</li> <li>Geospatial data</li> <li>Remote sensing data</li> </ul>"},{"location":"about/#industry-experience","title":"Industry Experience","text":"<p>Experience spans across: - University research - Non-government organizations - Start-ups - Private sector enterprises</p>"},{"location":"about/#research-interests","title":"Research Interests","text":"<ul> <li>Signal Processing</li> <li>Bayesian Methods</li> <li>Machine Learning Applications</li> <li>Remote Sensing</li> <li>Environmental Data Analysis</li> </ul>"},{"location":"education/","title":"Education","text":""},{"location":"education/#phd-in-applied-statistics-and-geo-computation","title":"Ph.D. in Applied Statistics and Geo-computation","text":"<p>Texas A&amp;M University | 2013 - 2017 - Focus: Signal processing, Bayesian and Machine learning - Research: LiDAR data processing and analysis - Publications in remote sensing and machine learning applications</p>"},{"location":"education/#ms-in-environmental-science","title":"M.S. in Environmental Science","text":"<p>Beijing Normal University | 2011 - 2013 - Focus: Environment Science analysis and modeling - Research: Environmental data analysis</p>"},{"location":"experience/","title":"Experience","text":""},{"location":"experience/#data-science-manager-technical-lead","title":"Data Science Manager (Technical lead)","text":"<p>Equinix, remote |  Jan 2024 - present</p> <p>Led enterprise HR bot development: Design and Enhanced GenAI based UX through design improvements, integrated Azure AI search, and optimized data ingestion, driving significant productivity gains and reducing HR support costs.</p> <p>Developed EIDP (GenAI-powered document processor): Automated extraction, analysis, summarization, and translation of diverse documents, streamlining processes across Equinix departments and boosting accuracy and efficiency.</p> <p>Built contract health monitoring overhaul: Leveraged GenAI to conduct contract analysis and comparison, cutting review time and costs while delivering actionable insights for risk management.</p>"},{"location":"experience/#lead-ml-engineer-lead-ds","title":"Lead ML Engineer/ Lead DS","text":"<p>Equinix, remote | Sep 2021 - Dec 2023</p> <p>Led B2B Intelligent Price Recommendations: Built AI/ML-driven pricing models for renewals and new deals with explainable AI, cutting manual effort, scaling operations, and increasing user trust &amp; understanding.</p> <p>Conducted LLM Fine-Tuning for Strategic Optimization: Fine-tuned GPT-3.5 and LLaMA models to compare fine-tuning vs RAG approaches, evaluating accuracy and cost to guide company AI strategies. Utilized Lora and custom evaluation frameworks to assess performance to provide clear insights for scalable AI adoption.</p> <p>Developed audio-based predictive maintenance system: Employed ML/ DL(CNN) to analyze audio data from IBX, enhancing safety by predicting failures, reducing downtime for improved productivity and customer experience.</p> <p>Designed and led sustainability dashboard development: Utilized ETS time series models to forecast carbon footprint, delivering insights for emission reduction targets, supporting climate neutrality commitment.</p>"},{"location":"experience/#senior-data-scientist","title":"Senior Data Scientist","text":"<p>Colaberry/Bayer Crop Science - St. Louis | July2018 - Agu 2021</p> <p>Built automated data pipeline and processing framework: Developed a PySpark-based system to process large geospatial data (e.g., Sentinel-2), applying ML (RF, Bayesian) methods to estimate soil attributes with uncertainty, and support field operation.</p> <p>Enhanced yield prediction with ML and sentiment analysis: Combined machine learning and SWIVEL pretrained embeddings to forecast and synchronize crop yields during the growing season, improving planning accuracy.</p> <p>Analyzed GET interactions for crop yield prediction: Built a framework to evaluate genetic, environmental, and treatment (e.g., seed rate) effects, enabling outcome-based pricing insights and crop hybrid optimization.</p>"},{"location":"experience/#postdoctoral-research-associate","title":"Postdoctoral Research Associate","text":"<p>Texas A&amp;M University | Jan 2018 - July 2018</p> <ul> <li>Developed validation plan for upcoming ICESat-2 data</li> <li>Developed algorithms for waveform LiDAR visualization and R package</li> <li>Predicted corn and sorghum yield with UAV-acquired data</li> </ul> <p>[Additional experience details...]</p>"},{"location":"skills/","title":"Skills","text":""},{"location":"skills/#languages-operating-systems-tools","title":"Languages, Operating Systems &amp; Tools","text":"<ul> <li>Python, R, SQL, JavaSCript, HTML, CSS</li> <li>git, linux, bash, Docker, Kubernetes</li> <li>GCP, AZURE (LLM based services), AWS, Pyspark</li> </ul>"},{"location":"skills/#genai","title":"GenAI","text":"<ul> <li>Langchain</li> <li>AZURE OpenAI, Azure Form Recognizer, Google Vertex AI</li> <li>Ollama, Deepseek, OpenCV</li> <li>RAG, Fine-tuning, Prompt Engineering, LLMs, AgentAI, deep research</li> </ul>"},{"location":"skills/#data-science","title":"Data Science","text":"<ul> <li>Data Cleaning</li> <li>Data Visualization</li> <li>Data Wrangling</li> <li>Data Analysis/modelling</li> </ul>"},{"location":"skills/#machine-learning","title":"Machine Learning","text":"<ul> <li>Bayesian</li> <li>Random Forest</li> <li>Neural Network &amp; Deep Learning (CNN, ResNet, UNet, LSTM, transfer learning)</li> <li>Decision Tree</li> <li>Nearest Neighbor</li> <li>Support Vector Machine</li> <li>Recommender System</li> <li>Natural Language Processing</li> </ul>"},{"location":"skills/#statistical-methods","title":"Statistical Methods","text":"<ul> <li>Partial Least Square Regression</li> <li>Univariate and Multivariate Regression</li> <li>Linear Discriminant Analysis</li> <li>Logistic Regression</li> <li>Time Series Analysis</li> <li>Factor Analysis</li> <li>Mixed Effect Modeling</li> </ul>"},{"location":"projects/","title":"Professional Projects","text":""},{"location":"projects/#recent-projects","title":"Recent Projects","text":""},{"location":"projects/#generative-ai-solutions","title":"Generative AI Solutions","text":"<ul> <li>Enterprise HR Chatbot Development</li> <li>Intelligent Document Processing System</li> <li>Contract Health Monitoring System</li> </ul>"},{"location":"projects/#machine-learning-applications","title":"Machine Learning Applications","text":"<ul> <li>Sentimental Analysis</li> <li>Enhanced Fraud Detection</li> <li>Time Series Analysis Basic</li> </ul>"},{"location":"projects/#deep-learning-advanced-analytics","title":"Deep Learning &amp; Advanced Analytics","text":"<ul> <li>Bayesian LSTM</li> <li>NLP SWIVEL</li> <li>Ensemble Models PySpark</li> <li>Bayesian Optimization Neural Networks</li> </ul>"},{"location":"projects/index_old/","title":"Projects Overview","text":"<p>My work spans across various domains of data science and machine learning, including:</p> <pre><code>- GenAI\n- Machine Learning and Deep Learning Applications\n- Natural Language Processing\n- Time Series Analysis\n- Computer Vision\n- LiDAR Data Processing\n</code></pre>"},{"location":"projects/contributions/","title":"Open Source Contributions","text":""},{"location":"projects/contributions/#lidar-data-processing-analysis","title":"LiDAR Data Processing &amp; Analysis","text":""},{"location":"projects/contributions/#waveformlidar-r-package","title":"Waveformlidar R Package","text":"<p>An R package dedicated to Full Waveform (FW) LiDAR data processing, analysis, and visualization. Provides tools for waveform decomposition, metrics extraction, and point cloud generation.</p> <p>Learn More</p>"},{"location":"projects/contributions/#tree-segmentation-algorithm-not-finised","title":"Tree Segmentation Algorithm (not finised)","text":"<p>Developed advanced algorithms for individual tree detection and segmentation using LiDAR point cloud data, incorporating machine learning techniques for improved accuracy.</p> <p>Learn More</p>"},{"location":"projects/contributions/tree_segmentation/","title":"Tree Segmentation Algorithm","text":""},{"location":"projects/contributions/tree_segmentation/#overview","title":"Overview","text":"<p>Advanced algorithm development for individual tree detection and segmentation using LiDAR point cloud data.</p>"},{"location":"projects/contributions/tree_segmentation/#technologies","title":"Technologies","text":"<ul> <li>R Programming</li> <li>Point Cloud Processing</li> <li>Machine Learning</li> <li>Spatial Analysis</li> </ul>"},{"location":"projects/contributions/tree_segmentation/#key-features","title":"Key Features","text":"<ul> <li>Automated tree detection</li> <li>Crown delineation</li> <li>Height estimation</li> <li>Biomass calculation</li> <li>Species classification</li> </ul>"},{"location":"projects/contributions/tree_segmentation/#results","title":"Results","text":"<ul> <li>Improved detection accuracy</li> <li>Reduced processing time</li> <li>Enhanced crown delineation</li> <li>Better species differentiation</li> </ul>"},{"location":"projects/contributions/tree_segmentation/#applications","title":"Applications","text":"<ul> <li>Forest inventory</li> <li>Urban tree mapping</li> <li>Ecological monitoring</li> <li>Forest management</li> </ul>"},{"location":"projects/contributions/waveformlidar_r_package/","title":"Waveformlidar R Package","text":"<p> Project link: https://github.com/tankwin08/waveformlidar</p>"},{"location":"projects/contributions/waveformlidar_r_package/#overview","title":"Overview","text":"<p>An open-source R package for processing waveform lidar data and exemplifying their uses in vegetation structure analysis.</p>"},{"location":"projects/contributions/waveformlidar_r_package/#key-features","title":"Key Features","text":"<ul> <li>Gaussian, adaptive Gaussian, and Weibull decompositions</li> <li>Deconvolution approaches (Gold and Richard-Lucy)</li> <li>Waveform metrics extraction</li> <li>Hyper Point Cloud (HPC) generation</li> <li>3D voxelization capabilities</li> <li>Composite waveform generation</li> </ul>"},{"location":"projects/contributions/waveformlidar_r_package/#applications","title":"Applications","text":"<ul> <li>Forest structure analysis</li> <li>Vegetation mapping</li> <li>Biomass estimation</li> <li>Ecological monitoring</li> <li>Environmental assessment</li> </ul>"},{"location":"projects/contributions/waveformlidar_r_package/#impact","title":"Impact","text":"<ul> <li>Simplified waveform processing workflow</li> <li>Enhanced data visualization</li> <li>Improved analysis capabilities</li> <li>Integration with existing LiDAR tools</li> </ul> <p>GitHub Repository</p>"},{"location":"projects/creations/","title":"Professional Projects","text":""},{"location":"projects/creations/#recent-projects","title":"Recent Projects","text":""},{"location":"projects/creations/#generative-ai-solutions","title":"Generative AI Solutions","text":"<ul> <li>Enterprise HR Chatbot Development</li> <li>Intelligent Document Processing System</li> <li>ML-Driven Predictive Maintenance</li> <li>Sustainability Analytics Dashboard</li> </ul>"},{"location":"projects/creations/#machine-learning-applications","title":"Machine Learning Applications","text":"<ul> <li>Sentimental Analysis</li> <li>Enhanced Fraud Detection</li> <li>Time Series Analysis Basic</li> </ul>"},{"location":"projects/creations/#deep-learning-advanced-analytics","title":"Deep Learning &amp; Advanced Analytics","text":"<ul> <li>Bayesian LSTM</li> <li>NLP SWIVEL</li> <li>Ensemble Models PySpark</li> <li>Bayesian Optimization Neural Networks</li> </ul>"},{"location":"projects/creations/ARIMA_vs._LSTM_time_series_data/","title":"Time series analysis using ARIMA &amp; LSTM - MODIS","text":"","tags":["Time series","ARIMA","Seasonal ARIMA","Stationary","LSTM","RISK prediction"]},{"location":"projects/creations/ARIMA_vs._LSTM_time_series_data/#introduction","title":"Introduction","text":"<p>project link</p> <p>To investigate the trend and pattern of time seriese data (MODIS data) using the Autoregressive Integrated Moving Averages (ARIMA) and Long Short Term Memory (LSTM) networks and further to check if we can use the current model to predict further values of target variables.</p>","tags":["Time series","ARIMA","Seasonal ARIMA","Stationary","LSTM","RISK prediction"]},{"location":"projects/creations/ARIMA_vs._LSTM_time_series_data/#arima-autoregressive-integrated-moving-average","title":"ARIMA (Autoregressive integrated moving average)","text":"<p>ARIMA can explain the time series pattern for given frequency or lag (hour, day and week ...) and also predict furhter values.</p> <p>ARIMA analysis will focus on the logic of model and how to select the three important terms of the model: p is the order of the AR term,</p> <p>q is the order of the MA term, d is the number of differencing required to make the time series stationary. You can go to here for  detailed explaination of ARIMA.</p> <p>The first step of the model is to make the time series data stationary. </p> <p>Stationarity</p> <p>what is stationary?</p> <p>1 The mean of the series should not be a function of time. </p> <p>2 The variance of the series should not be a function of time. This property is known as homoscedasticity. </p> <p>3 the covariance of the i th term and the (i + m) th term should not be a function of time</p> <p>why the data need to be stationary?</p> <p>Because, term \u2018Auto Regressive\u2019 in ARIMA means it is a linear regression model that uses its own lags as predictors. </p> <p>When running a linear regression the assumption is that all of the observations are all independent of each other. In a time series, however, we know that observations are time dependent.  It turns out that a lot of nice results that hold for independent random variables (law of large numbers and central limit theorem to name a couple) hold for stationary random variables.  So by making the data stationary, we can actually apply regression techniques to this time dependent variable.</p> <p>How to check Stationarity?</p> <p>An augmented Dickey\u2013Fuller test (ADF) tests the null hypothesis that  a unit root is present in a time series sample. The alternative hypothesis is different depending on which version of the test is used, but is usually stationarity or trend-stationarity.</p> <p>Basically, we are trying to whether to accept the Null Hypothesis H0 (that the time series has a unit root, indicating it is non-stationary) or reject H0 and go with the Alternative Hypothesis (that the time series has no unit root and is stationary).</p> <p>We end up deciding this based on the p-value return.</p> <pre><code>    \u2022A small p-value (typically \u2264 0.05) indicates strong evidence against the null hypothesis, so you reject the null hypothesis.\n\n    \u2022A large p-value (&gt; 0.05) indicates weak evidence against the null hypothesis, so you fail to reject the null hypothesis.\n</code></pre> <p>Parameters of ARIMA</p> <p>How to make data stationary and determine d?</p> <p>The most common way is to difference it by substracting the previous values from the current values. This is also how to determine the d. </p> <p>If d = 0, the input data of the model will be original data. If d = 1, it means we need to use the first difference of the time seriese.</p> <p>Next, what are the \u2018p\u2019 and \u2018q\u2019 terms?</p> <p>\u2018p\u2019 is the order of the \u2018Auto Regressive\u2019 (AR) term. It refers to the number of lags of Y to be used as predictors.  And \u2018q\u2019 is the order of the \u2018Moving Average\u2019 (MA) term. It refers to the number of lagged forecast errors that should go into the ARIMA Model.</p> <p>How to determine p and q?</p> <ul> <li> <p>Identification of an AR model is often best done with inspecting the Partial Autocorrelation (PACF) plot.</p> <ul> <li>For an AR model, the theoretical PACF \u201cshuts off\u201d past the order of the model.  The phrase \u201cshuts off\u201d means that in theory  the partial autocorrelations are equal to 0 beyond that point.  Put another way, the number of non-zero partial autocorrelations gives the order of the AR model.  By the \u201corder of the model\u201d we mean the most extreme lag of x that is used as a predictor.</li> </ul> <p>how to determine p: In particular, the partial autocorrelation at lag k is equal to the estimated AR(k) coefficient in an autoregressive model with k terms--i.e., a multiple regression model in which Y is regressed on LAG(Y,1), LAG(Y,2), etc., up to LAG(Y,k). Thus, by mere inspection of the PACF you can determine how many AR terms you need to use to explain the autocorrelation pattern in a time series: if the partial autocorrelation is significant at lag k and not significant at any higher order lags--i.e., if the PACF \"cuts off\" at lag k--then this suggests that you should try fitting an autoregressive model of order k</p> </li> <li> <p>Identification of an MA model is often best done with the ACF rather than the PACF.</p> <ul> <li>For an MA model, the theoretical PACF does not shut off, but instead tapers toward 0 in some manner.  A clearer pattern for an MA model is in the ACF.  The ACF will have non-zero autocorrelations only at lags involved in the model.</li> </ul> <p>how to determine q: to see first negative values of ACF plot's corresponding x-axis (lag) value</p> </li> </ul> <p>Note:  The sole reliance on the ACF and PACF to determine the p and q is NOT always correct. These plots just can help you understnad the ARIMA model and justify whether you can get a better model or worse one.  Just try the simple model first and find a better model using AIC through multiple time exploration.</p>","tags":["Time series","ARIMA","Seasonal ARIMA","Stationary","LSTM","RISK prediction"]},{"location":"projects/creations/Bayesian%20uncertainty%20of%20Neutral%20Networks%20%28LSTM%29%20for%20time%20series%20analysis/","title":"Bayesian Uncertainty for time series data (EVI) prediction using LSTM and autoencoder","text":"","tags":["Time series","LSTM","Bayesian Uncertainty","Autoencoder","Dropout","Model misspecification","Inherent noise"]},{"location":"projects/creations/Bayesian%20uncertainty%20of%20Neutral%20Networks%20%28LSTM%29%20for%20time%20series%20analysis/#overview","title":"Overview","text":"<p>To investigate the trend and pattern of time seriese data (MODIS data) using the Long Short Term Memory (LSTM) networks and quantify the uncertianty of the time series prediction of target variables. </p> <p>Project link</p> <p></p>","tags":["Time series","LSTM","Bayesian Uncertainty","Autoencoder","Dropout","Model misspecification","Inherent noise"]},{"location":"projects/creations/Bayesian%20uncertainty%20of%20Neutral%20Networks%20%28LSTM%29%20for%20time%20series%20analysis/#bayesian-nn","title":"Bayesian NN","text":"<p>How much confidence do you know about your model results or a paritcular prediction?</p> <p>This is a critical important question for many business. With the advent of deep learning, many forecasting problems for business  have been solved in innovative ways. For example, Uber researchers has provided a fascianting paper on time series prediction.</p> <p>Standard deep learning method such as LSTM do not capture model uncertianty. However, the uncertianty estimation is indispensable for deep learning models.</p> <p>Bayesian probability theory offers us mathematically grounded tools to reason about model uncertainty, but these usually come with a prohibitive computational cost [2]. </p> <p>In deep learning, there are two kinds of strategries to quantify the uncertianty: (1) MC dropout and (2) variational inference.</p> <p>(1) Regarding MC dropout, Gal developed a framework casting dropout training in deep neural networks (NNs) as approximate Bayesian inference in deep Gaussian processes. This method can mitigates the problem of representing model uncertainty in deep learning without sacrificing either computational complexity or test accuracy.</p> <p>(2)Variational inference such as sampling-based variational inference and stochastic variational inference has been applied to deep learning models, which have performed as well as dropout. However, this approach comes with a prohibitive computational cost. To represent uncertainty, the number of parameters in these appraoches is doubled for the same network size.  Further, they require more time to converge and do not improve on existing techniques. Given that good uncertainty estimates can be cheaply obtained from common dropout models,  this might result in unnecessary additional computation.</p> <p>what is variational inference? In short, variational inference is an approach of approximating model posterior which would otherwise be difficult to work with directly. Intuitively, this is a measure of similarity between the two distributions although it is not symmetric. So minimising this objective fits our approximating distribution  to the distribution we care about. </p> <p>This is standard in variational inference where we fit distributions rather than parameters, resulting in our robustness to over-fitting. </p>","tags":["Time series","LSTM","Bayesian Uncertainty","Autoencoder","Dropout","Model misspecification","Inherent noise"]},{"location":"projects/creations/Bayesian%20uncertainty%20of%20Neutral%20Networks%20%28LSTM%29%20for%20time%20series%20analysis/#how-dropout-to-represent-bayesian-approximation","title":"How dropout to represent Bayesian approximation","text":"<p>Compared to standard NN, the BNN added a binary vector in each layer.  We sample new realisations for the binary vectors bi for every input point and every forward pass thorough  the model (evaluating the model's output), and use the same values in the backward pass (propagating the derivatives to the parameters to be optimised W1,W2,b).  The elements of vector bi take value 1 with probability 0\u2264pi\u22641 for i=1,2...l. i is the ith layer.</p> <p>The dropped weights b1W1 and b2W2 are often scaled by 1/pi to maintain constant output magnitude. At test time we do not sample any variables and simply use  the full weights matrices W1,W2,b. </p> <p>Actually, the dropout network is similar to a Gaussian process approximation.  Different network structures and different non-linearities would correspond to different prior beliefs as to what we expect our uncertainty to look like.  This property is shared with the Gaussian process as well. Different Gaussian process covariance functions would result in different uncertainty estimates. If you are interested in more details on the BNN. Please refer to [here]\uff08http://mlg.eng.cam.ac.uk/yarin/blog_3d801aa532c1ce.html\uff09 - Why Does It Even Make Sense?</p>","tags":["Time series","LSTM","Bayesian Uncertainty","Autoencoder","Dropout","Model misspecification","Inherent noise"]},{"location":"projects/creations/Bayesian%20uncertainty%20of%20Neutral%20Networks%20%28LSTM%29%20for%20time%20series%20analysis/#calculating-prediction-uncertainty-with-bnns-developed-by-uber","title":"Calculating prediction uncertainty with BNNs developed by Uber","text":"<p>The variance quantifies the prediction uncertainty, which can be broken down using the law of total variance. An underlying assumption for the model uncertainty equation is that yhat is generated by the same procedure, but this is not always the case. In anomaly detection, for instance,  it is expected that certain time series will have patterns that differ greatly from the trained model.  Therefore, reseachers from Uber propose that a complete measurement of prediction uncertainty should be composed of three parts: </p> <p>1 model uncertainty</p> <p>2 model misspecification</p> <p>3 inherent noise level.</p>","tags":["Time series","LSTM","Bayesian Uncertainty","Autoencoder","Dropout","Model misspecification","Inherent noise"]},{"location":"projects/creations/Bayesian%20uncertainty%20of%20Neutral%20Networks%20%28LSTM%29%20for%20time%20series%20analysis/#1-model-uncertainty","title":"1 Model Uncertainty","text":"<p>Model uncertainty, also referred to as epistemic uncertainty, captures our ignorance of the model parameters and can be reduced as more samples are collected.  The key to estimating model uncertainty is the posterior distribution , also referred to as Bayesian inference.  This is particularly challenging in neural networks because of the non-conjugacy often caused by nonlinearities.</p> <p>Here we used Monte Carlo dropout to approximate model uncertainty.</p>","tags":["Time series","LSTM","Bayesian Uncertainty","Autoencoder","Dropout","Model misspecification","Inherent noise"]},{"location":"projects/creations/Bayesian%20uncertainty%20of%20Neutral%20Networks%20%28LSTM%29%20for%20time%20series%20analysis/#2-model-misspecification","title":"2 Model misspecification","text":"<p>Model misspecification captures the scenario where testing samples come from a different population than the training set, which is often the case in time series anomaly detection.  Similar concepts have gained attention in deep learning under the concept of adversarial examples in computer vision, but its implication in prediction uncertainty remains relatively unexplored</p> <p>Here we first fit a latent embedding space for all training time series using an encoder-decoder framework.  From there, we are able to measure the distance between test cases and training samples in the embedded space.</p> <p>After estimating uncertaitny from model misspecification, we combined model uncertianty with model misspecification uncertianty by connecting the encoder-decoder network with a prediction network, and treat them as one large network during inference.</p>","tags":["Time series","LSTM","Bayesian Uncertainty","Autoencoder","Dropout","Model misspecification","Inherent noise"]},{"location":"projects/creations/Bayesian%20uncertainty%20of%20Neutral%20Networks%20%28LSTM%29%20for%20time%20series%20analysis/#3-inherent-noise","title":"3 Inherent noise","text":"<p>Inherent noise is mainly to capture the uncertainty in the data generation process and which is irreducible.  Uber reseachers propose a simple but adaptive approach by estimating the noise level via the residual sum of squares, evaluated on an independent held-out validation set.</p> <p>If you are interested in technical parts on these three sections, you can go to here for more details.</p>","tags":["Time series","LSTM","Bayesian Uncertainty","Autoencoder","Dropout","Model misspecification","Inherent noise"]},{"location":"projects/creations/bayesian_nn/","title":"Bayesian Neural Networks","text":""},{"location":"projects/creations/bayesian_nn/#overview","title":"Overview","text":"<p>Implemented Bayesian Neural Networks for probabilistic deep learning, providing uncertainty estimates in predictions.</p>"},{"location":"projects/creations/bayesian_nn/#technologies","title":"Technologies","text":"<ul> <li>PyMC3</li> <li>TensorFlow Probability</li> <li>Probabilistic Programming</li> <li>Deep Learning</li> </ul>"},{"location":"projects/creations/bayesian_nn/#key-features","title":"Key Features","text":"<ul> <li>Uncertainty quantification</li> <li>Posterior distribution estimation</li> <li>Variational inference</li> <li>Model averaging</li> <li>Active learning integration</li> </ul>"},{"location":"projects/creations/bayesian_nn/#applications","title":"Applications","text":"<ul> <li>Risk assessment</li> <li>Decision making under uncertainty</li> <li>Anomaly detection</li> <li>Predictive maintenance</li> </ul>"},{"location":"projects/creations/bayesian_optimziation_NN/","title":"Bayesian optimization deep learning","text":"<p>project link</p>","tags":["Neutral Networks","Bayesian optimization","Classification","Architecture Optimization"]},{"location":"projects/creations/bayesian_optimziation_NN/#introduction","title":"Introduction:","text":"<p>The picture of Bayesian optimization is obtianed from here</p> <p></p> <p>Bayesian optimization</p> <p>There are a lot of hyperparameters for machine learning models such as NN. Typically, random or grid search are effecient ways to conduct the optimization of models. They can be very time-consuming in some cases which waste time on unpromising areas of search space. Bayesian optimization can overcome this problem by adopting an informed seach method in the space to find the optmized parameters.</p> <p>Bayesian optimization works by constructing a posterior distribution of functions (gaussian process) that best describes the function you want to optimize. As the number of observations grows, the posterior distribution improves, and the algorithm becomes more certain of which regions in parameter space are worth exploring and which are not. You can find more information and explination here</p> <p>It's worthy to note that the Bayesian optimization is to find the maximum of a function. Thus, when we formulate a function and evaulation metrics, we should take this part into consideration. For example, when we used log loss to evaluate our model performance, the smaller values will be better. We should return a negative logloss to make it suitable for maximum of the defined function.</p> <p>Note: It took a long time to run if you have a big dataset and wide boundary. You can refer to Colab for running the code.</p>","tags":["Neutral Networks","Bayesian optimization","Classification","Architecture Optimization"]},{"location":"projects/creations/bayesian_optimziation_NN/#technologies","title":"Technologies","text":"<ul> <li>PyMC3</li> <li>TensorFlow Probability</li> <li>Probabilistic Programming</li> <li>Deep Learning</li> </ul>","tags":["Neutral Networks","Bayesian optimization","Classification","Architecture Optimization"]},{"location":"projects/creations/bayesian_optimziation_NN/#key-features","title":"Key Features","text":"<ul> <li>Uncertainty quantification</li> <li>Posterior distribution estimation</li> <li>Variational inference</li> <li>Model averaging</li> <li>Active learning integration</li> </ul>","tags":["Neutral Networks","Bayesian optimization","Classification","Architecture Optimization"]},{"location":"projects/creations/bayesian_optimziation_NN/#applications","title":"Applications","text":"<ul> <li>Risk assessment</li> <li>Decision making under uncertainty</li> <li>Anomaly detection</li> <li>Predictive maintenance</li> </ul>","tags":["Neutral Networks","Bayesian optimization","Classification","Architecture Optimization"]},{"location":"projects/creations/enhanced_fraud_detection/","title":"Enhanced Fraud Detection System","text":"<p> Project link</p>"},{"location":"projects/creations/enhanced_fraud_detection/#overview","title":"Overview","text":"<p>Developed a scalable fraud detection system using PySpark to handle large-scale transaction data and identify fraudulent activities in real-time.</p>"},{"location":"projects/creations/enhanced_fraud_detection/#technologies","title":"Technologies","text":"<ul> <li>PySpark MLlib</li> <li>Distributed Computing</li> <li>Machine Learning Pipeline</li> <li>Real-time Processing</li> </ul>"},{"location":"projects/creations/enhanced_fraud_detection/#key-features","title":"Key Features","text":"<ul> <li>Automated feature engineering</li> <li>Imbalanced data handling</li> <li>Real-time prediction capabilities</li> <li>Scalable processing pipeline</li> <li>Model performance monitoring</li> </ul>"},{"location":"projects/creations/enhanced_fraud_detection/#results","title":"Results","text":"<ul> <li>Reduced false positives by 30%</li> <li>Improved detection rate by 25%</li> <li>Processing capability of 1M+ transactions/hour</li> </ul>"},{"location":"projects/creations/enhanced_fraud_detection/#goal","title":"Goal","text":"<p>To examplify the feature selection strategy in PySpark and furhter enhanc the pyspark pipeline\u2019s performance on fraud detection.</p> <p>In this project, I will continue to work on the data from the project fradu_detection_ML_PySpark. The data exploration will be same, and the feature selction will use input perturbation strtegry instead of PCA as I did in the previous project.</p>"},{"location":"projects/creations/enhanced_fraud_detection/#why-feature-selection","title":"Why feature selection?","text":"<ol> <li>Curse of dimensionality \u2014 Overfitting</li> </ol> <p>The common theme of the problems is that when the dimensionality increases, the volume of the space increases so fast that the available data become sparse. This sparsity is problematic for any method that requires statistical significance.</p> <p>In order to obtain a statistically sound and reliable result, the amount of data needed to support the result often grows exponentially with the dimensionality.</p> <p>Also, organizing and searching data often relies on detecting areas where objects form groups with similar properties; in high dimensional data, however, all objects appear to be sparse and dissimilar in many ways, which prevents common data organization strategies from being efficient.</p> <ol> <li>Occam\u2019s Razor</li> </ol> <p>We want our models to be simple and explainable. We lose explainability when we have a lot of features.</p> <p>3 Noise in the data</p> <p>In real applications, the data are not perfect and always noisy inherently.</p> <p>Commonly used methods for feature selection In summary, there are several commonly used methods to conduct feature selction in data preprocessing.</p> <p>1 Correlation or chi-square</p> <p>Chose the top-n high correlated variables or high chi-squre variables with respective to target variables. The intuition is that if a feature is independent to the target, it will not be useful or uninformative for target classification or regression.</p> <p>2 Stepwise method</p> <p>This is a wrapper based method. The goal of recursive feature elimination (RFE) is to select features by recursively considering smaller and smaller sets of features. First, the estimator is trained on the initial set of features and the importance of each feature is obtained either through a coef_ attribute or through a feature_importances_ attribute. Then, the least important features are pruned from current set of features. That procedure is recursively repeated on the pruned set until the desired number of features to select is eventually reached.</p> <p>3 Lasso - Penalized likelihood</p> <p>LASSO models have been used extensively in high-dimensional model selection problems, that is when the number of IVs \ud835\udc58 by far exceeds the sample size \ud835\udc5b.</p> <p>Regression coefficients estimated by the LASSO are biased by intention, but can have smaller mean squared error (MSE) than conventional estimates. It prefer to have fewer variales with huge contribution to the target.</p> <p>Because of the bias, their interpretation in explanatory or descriptive models is difficult, and confidence intervals based on resampling procedures such as the percentile bootstrap do not reach their claimed nominal level. Another problem with LASSO estimation is its dependence on the scale of the covariates</p> <p>4 PCA</p> <p>PCA is a commonly used as dimension reduction technique by projecting each data point onto only the first few principal components to obtain lower-dimensional data while preserving as much of the data\u2019s variation as possible.</p> <p>The advantages of PCA:</p> <p>Removes Correlated Features Improves Algorithm Performance Reduces Overfitting Improves Visualization The things we need to consider before using PCA:</p> <p>Independent variables become less interpretable: Data standardization is must before PCA: pca is affected by scale Information loss 5 Input Perturbation</p> <p>This algorithm was introduced by Breiman in his seminal paper on random forests. Although he presented this algorithm in conjunction with random forests, it is model-independent and appropriate for any supervised learning model.</p> <p>This algorithm, known as the input perturbation algorithm, works by evaluating a trained model\u2019s accuracy with each of the inputs individually shuffled from a data set. Shuffling an input causes it to become useless\u2014effectively removing it from the model. More important inputs will produce a less accurate score when they are removed by shuffling them. This process makes sense, because important features will contribute to the accuracy of the model.</p>"},{"location":"projects/creations/ensemble_model_pyspark/","title":"Advanced Ensemble Learning Models in pyspark","text":"<p>project link</p>"},{"location":"projects/creations/ensemble_model_pyspark/#overview","title":"Overview","text":"<p>To examplify the uses of ensemble models in PySpark as the ensemble models in previous project using sklearn and keras and predict if the client will subscribe (yes/no) a term deposit (variable y) using market campaign data.</p> <p></p>"},{"location":"projects/creations/ensemble_model_pyspark/#technologies","title":"Technologies","text":"<ul> <li>Scikit-learn</li> <li>XGBoost</li> <li>LightGBM</li> <li>CatBoost</li> <li>Stacking/Blending techniques</li> </ul>"},{"location":"projects/creations/ensemble_model_pyspark/#key-features","title":"Key Features","text":"<ul> <li>Model stacking</li> <li>Cross-validation</li> <li>Feature importance analysis</li> <li>Automated model selection</li> <li>Hyperparameter optimization</li> </ul>"},{"location":"projects/creations/ensemble_model_pyspark/#results","title":"Results","text":"<ul> <li>15% accuracy improvement</li> <li>Enhanced model robustness</li> <li>Reduced overfitting</li> </ul>"},{"location":"projects/creations/ensemble_model_pyspark/#ensemble-models","title":"Ensemble models","text":"<p>Ensemble modeling is a process where multiple diverse models are created to predict an outcome, either by using many different modeling algorithms or using different training data sets. The ensemble model then aggregates the prediction of each base model and results in once final prediction for the unseen data. The motivation for using ensemble models is to reduce the generalization error of the prediction. As long as the base models are diverse and independent, the prediction error of the model decreases when the ensemble approach is used.</p> <p>The approach seeks the wisdom of crowds in making a prediction. Even though the ensemble model has multiple base models within the model, it acts and performs as a single model.</p> <p>A signle model generally suffers from high bias or high variance due to data quality, train and test data drift, distribution of hypothesis...</p> <p>The goal of the modelling to find a method wit low bias and low variance. Thus, it is common to aggregate several base models to provide solutions.</p>"},{"location":"projects/creations/ensemble_model_pyspark/#aggregate-strategies","title":"Aggregate strategies","text":"<p>There are multiple ways to conduct aggregation and improve the model performance either from accuracy or robustness. </p> <p>1 Baggging</p> <p>The bagging strategy is built on the bootstrap sampling. In short, it built multiple classifer independently and in parallel using data derived from resampling from the training set. Then aggregate these classifiers using average processing or major vote to redce the variability of prediction. However the accuracy/point estimate is not improved.</p> <p>2 Boosting</p> <p>The boosting is similar to bagging strategy to some extent in terms of resampling methods. But it differs in two major ways:</p> <pre><code>1 how trees are built: The Bagging method builds each tree independently while Boosting method builds one tree at a time. This additive model (ensemble) works in a forward stage-wise manner, introducing a weak learner to improve the shortcomings of existing weak learners.\n\n2 Results combination: The Bagging method combine results at the end of the process (by averaging or \"majority rules\") while the boosting combines results along the way.\n</code></pre> <p>If you carefully tune parameters, boosting can result in better performance than bagging. However, boosting may not be a good choice if you have a lot of noise, as it can result in overfitting. They also tend to be harder to tune than bagging method.</p> <p>3 Stacking</p> <p>Stacking provide a whole new different way to combine classifers. There are two major differences:</p> <pre><code>1 stacking often considers heterogeneous weak learners (different learning algorithms are combined) whereas bagging and boosting consider mainly homogeneous weak learners.\n\n2 The stacking uses a second layer model which uses the predictions of weak classifiers such as bagging and bostting results as input.\n</code></pre> <p>In this project, the stacking strategy was used to predict if the client will subscribe (yes/no) a term deposit (variable y) using market campaign data.</p>"},{"location":"projects/creations/ensemble_model_pyspark/#data","title":"Data","text":"<p>The data used for this project can be downloaded from here.</p> <p>The explination of the data can be found here.</p> <p>The data is related with direct marketing campaigns of a Portuguese banking institution. The marketing campaigns were based on phone calls. Often, more than one contact to the same client was required, in order to access if the product (bank term deposit) would be ('yes') or not ('no') subscribed. </p>"},{"location":"projects/creations/ensemble_model_pyspark/#input-variables","title":"Input variables:","text":"<p>bank client data:</p> <pre><code>1 - age (numeric)\n2 - job : type of job (categorical: 'admin.','blue-collar','entrepreneur','housemaid','management','retired','self-employed','services','student','technician','unemployed','unknown')\n3 - marital : marital status (categorical: 'divorced','married','single','unknown'; note: 'divorced' means divorced or widowed)\n4 - education (categorical: 'basic.4y','basic.6y','basic.9y','high.school','illiterate','professional.course','university.degree','unknown')\n5 - default: has credit in default? (categorical: 'no','yes','unknown')\n6 - housing: has housing loan? (categorical: 'no','yes','unknown')\n7 - loan: has personal loan? (categorical: 'no','yes','unknown')\n</code></pre> <p>related with the last contact of the current campaign:</p> <pre><code>8 - contact: contact communication type (categorical: 'cellular','telephone') \n9 - month: last contact month of year (categorical: 'jan', 'feb', 'mar', ..., 'nov', 'dec')\n10 - day_of_week: last contact day of the week (categorical: 'mon','tue','wed','thu','fri')\n11 - duration: last contact duration, in seconds (numeric).\n</code></pre> <p>Important note: Attribute 11 highly affects the output target (e.g., if duration=0 then y='no'). Yet, the duration is not known before a call is performed. Also, after the end of the call y is obviously known. Thus, this input should only be included for benchmark purposes and should be discarded if the intention is to have a realistic predictive model.</p> <p>other attributes:</p> <pre><code>12 - campaign: number of contacts performed during this campaign and for this client (numeric, includes last contact)\n13 - pdays: number of days that passed by after the client was last contacted from a previous campaign (numeric; 999 means client was not previously contacted)\n14 - previous: number of contacts performed before this campaign and for this client (numeric)\n15 - poutcome: outcome of the previous marketing campaign (categorical: 'failure','nonexistent','success')\n</code></pre> <p>social and economic context attributes</p> <pre><code>16 - emp.var.rate: employment variation rate - quarterly indicator (numeric)\n17 - cons.price.idx: consumer price index - monthly indicator (numeric) \n18 - cons.conf.idx: consumer confidence index - monthly indicator (numeric) \n19 - euribor3m: euribor 3 month rate - daily indicator (numeric)\n20 - nr.employed: number of employees - quarterly indicator (numeric)\n</code></pre> <p>Output variable (desired target):</p> <pre><code>21 - y - has the client subscribed a term deposit? (binary: 'yes','no')\n</code></pre>"},{"location":"projects/creations/ensemble_model_pyspark/#references","title":"References","text":"<p>Machine Learning Case Study with Spark: Make it better.</p>"},{"location":"projects/creations/intelligent_document_processing_system.md/","title":"Intelligent Document Processing System","text":"<p>Project link</p>"},{"location":"projects/creations/intelligent_document_processing_system.md/#overview","title":"Overview","text":"<p>The Intelligent Document Processing System is a powerful application that leverages cutting-edge generative AI techniques to extract, analyze, and interact with information from various document types. This system transforms the way organizations handle document-based information by enabling natural language queries against document content.</p> <p></p>"},{"location":"projects/creations/intelligent_document_processing_system.md/#key-features","title":"Key Features","text":"<ul> <li>Multi-format Document Support: Process PDFs, Word documents, text files, CSVs, PowerPoint presentations, and even images</li> <li>Interactive Question-Answering: Ask natural language questions about document content</li> <li>Batch Processing: Run multiple queries against document sets simultaneously</li> <li>Flexible Model Selection: Choose from various LLM models based on specific needs</li> </ul>"},{"location":"projects/creations/intelligent_document_processing_system.md/#generative-ai-techniques-implemented","title":"Generative AI Techniques Implemented","text":""},{"location":"projects/creations/intelligent_document_processing_system.md/#retrieval-augmented-generation-rag","title":"Retrieval Augmented Generation (RAG)","text":"<p>At the core of our system is the RAG architecture, which combines the power of retrieval-based and generative approaches:</p> <ol> <li>Document Chunking &amp; Embedding: Documents are split into semantic chunks and transformed into vector embeddings using Ollama's embedding models</li> <li>Similarity-Based Retrieval: When a query is received, the system identifies the most relevant document sections using similarity search with threshold filtering</li> <li>Context-Aware Generation: The retrieved context is fed to a large language model along with the query to generate accurate, contextually relevant responses</li> </ol>"},{"location":"projects/creations/intelligent_document_processing_system.md/#advanced-prompt-engineering","title":"Advanced Prompt Engineering","text":"<p>The system employs sophisticated prompt templates that:</p> <ul> <li>Instruct the model to focus only on information present in the documents</li> <li>Provide clear instructions for handling cases where information isn't available</li> <li>Structure the context and question format for optimal model comprehension</li> </ul>"},{"location":"projects/creations/intelligent_document_processing_system.md/#vector-store-optimization","title":"Vector Store Optimization","text":"<p>Our implementation includes:</p> <ul> <li>Similarity Score Thresholding: Only returns documents above a relevance threshold</li> <li>Fetch-K Optimization: Retrieves a larger initial candidate set before filtering</li> <li>Optimized K-Parameter: Fine-tuned for the ideal balance between comprehensive context and focused responses</li> </ul>"},{"location":"projects/creations/intelligent_document_processing_system.md/#technical-implementation","title":"Technical Implementation","text":"<p>The system is built using:</p> <ul> <li>LangChain: For document processing, embedding, and retrieval pipelines</li> <li>Ollama: For local deployment of powerful LLMs like Deepseek</li> <li>Streamlit: For an intuitive, user-friendly interface</li> </ul>"},{"location":"projects/creations/intelligent_document_processing_system.md/#business-impact","title":"Business Impact","text":"<p>This intelligent document processing system delivers significant value by:</p> <ul> <li>Reducing time spent searching through documents by up to 70%</li> <li>Enabling natural language interaction with document repositories</li> <li>Providing accurate, contextual responses based on document content</li> <li>Supporting decision-making with rapid information retrieval</li> </ul>"},{"location":"projects/creations/intelligent_document_processing_system.md/#future-enhancements","title":"Future Enhancements","text":"<p>We're continuously improving the system with plans to add:</p> <ul> <li>Multi-modal document understanding (images within documents)</li> <li>Custom fine-tuning options for domain-specific applications</li> <li>Enhanced metadata filtering and search</li> </ul> <p>This project demonstrates the practical application of generative AI techniques to solve real-world document processing challenges, creating a powerful tool for knowledge extraction and information retrieval.</p>"},{"location":"projects/creations/nlp_swivel/","title":"Sentiment analysis for review classification using SWIVEL and a small datasets","text":"<p>Introduction: project link</p> <p>Genealy, it require an amount of data to train and a long time to train a NLP model for a specific dataset.  Transfer learning is commonly used in this case to conduct the sentiment analsis for Natural Language Processing (NLP) problem.</p> <p>Swivel performs approximate factorization of the point-wise mutual information matrix via stochastic gradient descent.  It uses a piecewise loss with special handling for unobserved co-occurrences, and thus makes use of all the information in the matrix. </p>","tags":["NLP","SWIVEL","Transfer learning","Embedding layer","Sentiment analysis"]},{"location":"projects/creations/nlp_swivel/#pretrained-word-embeddings","title":"Pretrained Word Embeddings","text":"<p>Pretrained Word Embeddings are the embeddings learned in one task that are used for solving another similar task.</p> <p>These embeddings are trained on large datasets, saved, and then used for solving other tasks. That\u2019s why pretrained word embeddings are a form of Transfer Learning.</p> <p>CBOW (Continuous Bag Of Words) and Skip-Gram are two most popular frames for word embedding. In CBOW the words occurring in context (surrounding words) of a selected word are used as inputs and middle or selected word as the target. Its the other way round in Skip-Gram, here the middle word tries to predict the words coming before and after it.</p> <p>Why do we need Pretrained Word Embeddings?</p> <p>Pretrained word embeddings capture the semantic and syntactic meaning of a word as they are trained on large datasets. They are capable of boosting the performance of a Natural Language Processing (NLP) model. These word embeddings come in handy during hackathons and of course, in real-world problems as well.</p> <p>But why should we not learn our own embeddings? Well, learning word embeddings from scratch is a challenging problem due to two primary reasons:</p> <ul> <li> <p>1 Sparsity of training data</p> </li> <li> <p>2 Large number of trainable parameters</p> </li> </ul>","tags":["NLP","SWIVEL","Transfer learning","Embedding layer","Sentiment analysis"]},{"location":"projects/creations/sentimental_analysis/","title":"Airline Sentiment Analysis","text":"<p> Project link</p>"},{"location":"projects/creations/sentimental_analysis/#overview","title":"Overview","text":"<p>Developed a sentiment analysis model to analyze customer feedback from airline tweets, classifying sentiments as positive, neutral, or negative.</p>"},{"location":"projects/creations/sentimental_analysis/#technologies-used","title":"Technologies Used","text":"<ul> <li>BERT for text classification</li> <li>Python (scikit-learn, transformers)</li> <li>Natural Language Processing</li> <li>Data Visualization (WordCloud)</li> </ul>"},{"location":"projects/creations/sentimental_analysis/#key-features","title":"Key Features","text":"<ul> <li>Pre-trained BERT model fine-tuning</li> <li>Multi-class sentiment classification</li> <li>Real-time sentiment prediction</li> <li>Interactive visualization dashboard</li> <li>Comparative model performance analysis</li> </ul>"},{"location":"projects/creations/sentimental_analysis/#results","title":"Results","text":"<ul> <li>Achieved 87% accuracy in sentiment classification</li> <li>Successfully identified key customer pain points</li> <li>Generated actionable insights for service improvement</li> </ul>"},{"location":"projects/creations/sentimental_analysis/#introduction","title":"Introduction","text":"<p>Note: Retraining the BERT model took a long time using a local computer, to run in the Google Colab will be a good choice</p>"},{"location":"projects/creations/sentimental_analysis/#objectives","title":"Objectives","text":"<p>To predict sentiment (postive, neutral, negeative) of customer feedback using tweet texts of differnt airline companies and compare different models\u2019performace on text classification.</p> <p>Specifically, multiple machine learing models such as KNN, Random forest and SVC have been used for conduct classification as a baseline. A new language representation model named BERT (Bidirectional Encoder Representations from Transformers) was also implemented to conduct sentiment analysis.</p>"},{"location":"projects/creations/sentimental_analysis/#bert","title":"BERT","text":"<p>Bidirectional Encoder Representations from Transformers (BERT) is a technique for NLP (Natural Language Processing) pre-training developed by Google and published in 2018.</p> <p>BERT is a method of pretraining language representations that was used to create models that NLP practicioners can then download and use for free. You can either use these models to extract high quality language features from your text data, or you can fine-tune these models on a specific task (classification, entity recognition, question answering, etc.) with your own data to produce state of the art predictions.</p>"},{"location":"projects/creations/sentimental_analysis/#how-bert-works","title":"How BERT works?","text":"<p>BERT was built on the Transformer, an attention mechanism that learns contextual relations between words (or sub-words) in a text. The attention mechanism was used to extratct information of context of a given words and then encode it in a learned vector. Generally, there are two mechanisms - an encoder that reads the text input and a decoder that produces a prediction for the task.</p> <p>The detailed workings of Transformer are described in a paper by Google. The figure below describe the brief steps of BERT during the traning process.</p> <p>where the model takes a pair of sequences and pools the representation of the first token in the sequence. Note that the original BERT model was trained for a masked language model and next-sentence prediction tasks, which includes layers for language model decoding and classification. These layers will not be used for fine-tuning the sentence pair classification.</p> <p>To help the model distinguish between the two sentences in training, the input is processed in the following way before entering the model:</p> <p>1 A [CLS] token is inserted at the beginning of the first sentence and a [SEP] token is inserted at the end of each sentence. 2 A sentence embedding indicating Sentence A or Sentence B is added to each token. Sentence embeddings are similar in concept to token embeddings with a vocabulary of 2. 3 A positional embedding is added to each token to indicate its position in the sequence. The concept and implementation of positional embedding are presented in the Transformer paper. Why BERT? 1 BERT offers an advantage over models like Word2Vec, because while each word has a fixed representation under Word2Vec regardless of the context within which the word appears, BERT produces word representations that are dynamically informed by the words around them.</p> <p>2 As opposed to directional models, which read the text input sequentially (left-to-right or right-to-left), the Transformer encoder reads the entire sequence of words at once. Therefore it is considered bidirectional, though it would be more accurate to say that it\u2019s non-directional. This characteristic allows the model to learn the context of a word based on all of its surroundings (left and right of the word).</p>"},{"location":"projects/creations/sentimental_analysis/#data","title":"Data","text":"<p>You can go to here to download the data used for the project.</p> <p>The deep learning and machine learning to conduct multi-class classification of text can be found here</p>"},{"location":"projects/creations/sentimental_analysis/#refereence","title":"Refereence","text":"<p>1 BERT Word Embeddings Tutorial</p> <p>2 BERT Explained: State of the art language model for NLP</p>"},{"location":"projects/creations/time_series_analysis/","title":"Time Series Analysis","text":""},{"location":"projects/creations/time_series_analysis/#overview","title":"Overview","text":"<p>Implemented advanced time series analysis models for forecasting and anomaly detection in business metrics.</p>"},{"location":"projects/creations/time_series_analysis/#technologies","title":"Technologies","text":"<ul> <li>ARIMA/SARIMA</li> <li>Prophet</li> <li>LSTM</li> <li>Statistical Methods</li> <li>Bayesian Analysis</li> </ul>"},{"location":"projects/creations/time_series_analysis/#key-features","title":"Key Features","text":"<ul> <li>Multi-step forecasting</li> <li>Seasonality detection</li> <li>Anomaly identification</li> <li>Trend analysis</li> <li>Uncertainty quantification</li> </ul>"},{"location":"projects/creations/time_series_analysis/#applications","title":"Applications","text":"<ul> <li>Sales forecasting</li> <li>Demand prediction</li> <li>Resource optimization</li> <li>Market trend analysis</li> </ul>"},{"location":"projects/creations/waveform_processing/","title":"Waveform Processing","text":""},{"location":"projects/creations/waveform_processing/#overview","title":"Overview","text":"<p>Advanced signal processing techniques for waveform data analysis and feature extraction.</p>"},{"location":"projects/creations/waveform_processing/#technologies","title":"Technologies","text":"<ul> <li>Signal Processing</li> <li>Python Scientific Stack</li> <li>Machine Learning</li> <li>Statistical Analysis</li> </ul>"},{"location":"projects/creations/waveform_processing/#key-features","title":"Key Features","text":"<ul> <li>Waveform decomposition</li> <li>Feature extraction</li> <li>Pattern recognition</li> <li>Noise reduction</li> <li>Real-time processing</li> </ul>"},{"location":"projects/creations/waveform_processing/#applications","title":"Applications","text":"<ul> <li>LiDAR data analysis</li> <li>Acoustic signal processing</li> <li>Sensor data analysis</li> <li>Pattern detection</li> </ul>"},{"location":"publications/","title":"Publications","text":""},{"location":"publications/#academic-papers","title":"Academic Papers","text":"<ul> <li> <p>waveformlidar: An R Package for Waveform LiDAR Processing and Analysis</p> </li> <li> <p>Photon counting LiDAR: An adaptive ground and canopy height retrieval algorithm for ICESat-2 data</p> </li> <li> <p>Bayesian and Classical Machine Learning Methods: A Comparison for Tree Species Classification with LiDAR Waveform Signatures</p> </li> <li> <p>Gold-A novel deconvolution algorithm with optimization for waveform LiDAR processing</p> </li> <li> <p>Bayesian Decomposition of Full Waveform LiDAR Data with Uncertainty Analysis</p> </li> <li> <p>Detecting and Quantifying Standing Dead Tree Structural Loss with Reconstructed Tree Models Using Voxelized Terrestrial Lidar Data</p> </li> <li> <p>A Deep Learning Semantic Segmentation-Based Approach for Field-Level Sorghum Panicle Counting</p> </li> <li> <p>Mapping forest aboveground biomass with a simulated ICESat-2 vegetation canopy product and Landsat data</p> </li> <li> <p>Estimating aboveground biomass and forest canopy cover with simulated ICESat-2 data and Landsat</p> </li> </ul>"},{"location":"publications/Bayesian_machine_learning_new/","title":"Bayesian and Classical Machine Learning Methods: A Comparison for Tree Species Classification with LiDAR Waveform Signatures","text":"<p>Introduction:</p> <p>A plethora of information contained in full-waveform (FW) Light Detection and Ranging (LiDAR) data offers prospects for characterizing vegetation structures. </p> <p>Goal:</p> <p>This study aims to investigate the capacity of FW LiDAR data alone for tree species identification through the integration of waveform metrics with machine learning methods and Bayesian inference.</p> <p>Methods:</p> <p>Specifically, we first conducted automatic tree segmentation based on the waveform-based canopy height model (CHM) using three approaches including TreeVaW, watershed algorithms and the combination of TreeVaW and watershed (TW) algorithms. </p> <p>Subsequently, the Random forests (RF) and Conditional inference forests (CF) models were employed to identify important tree-level waveform metrics derived from three distinct sources, such as raw waveforms, composite waveforms, the waveform-based point cloud and the combined variables from these three sources.</p> <p>Further, we discriminated tree (gray pine, blue oak, interior live oak) and shrub species through the RF, CF and Bayesian multinomial logistic regression (BMLR) using important waveform metrics identified in this study.</p> <p>Highlights:</p> <p>(1) tree segmentation: </p> <p>Results of the tree segmentation demonstrated that the TW algorithms outperformed other algorithms for delineating individual tree crowns. </p> <p>(2) RF vs. CF</p> <p>The CF model overcomes waveform metrics selection bias caused by the RF model which favors correlated metrics and enhances the accuracy of subsequent classification.</p> <p>(3) Composite waveform vs. raw waveform</p> <p>We also found that composite waveforms are more informative than raw waveforms and waveform-based point cloud for characterizing tree species in our study area.</p> <p>(4) RF vs. Bayesian </p> <p>Both classicalmachine learning methods (the RF and CF) and the BMLR generated satisfactory average overall accuracy (74% for the RF, 77% for the CF and 81% for the BMLR) and the BMLR slightly outperformed the other two methods. </p> <p>However, these three methods suffered from low individual classification accuracy for the blue oak which is prone to being misclassified as the interior live oak due to the similar characteristics of blue oak and interior live oak. </p> <p>Uncertainty estimates from the BMLR method compensate for this downside by providing classification results in a probabilistic sense and rendering users with more confidence in interpreting and applying classification results to real-world tasks such as forest inventory.</p> <p>(5) feature selection:</p> <p>Overall, this study recommends the CF method for feature selection and suggests that BMLR could be a superior alternative to classical machining learning methods.</p>","tags":["Bayesian multinomial logistic regression","Conditional inference forests","Machine learning, Random forests","Waveform signatures","Tree segmentation","Watershed","composite waveform"]},{"location":"publications/Gold/","title":"Gold-A novel deconvolution algorithm with optimization for waveform LiDAR processing","text":"<p>Introduction:</p> <p>Waveform Light Detection and Ranging (LiDAR) data have advantages over discrete-return LiDAR data inaccurately characterizing vegetation structure. However, we lack a comprehensive understanding ofwaveform data processing approaches under different topography and vegetation conditions. </p> <p>Goal:</p> <p>The objec-tive of this paper is to highlight a novel deconvolution algorithm, the Gold algorithm, for processingwaveform LiDAR data with optimal deconvolution parameters. Further, we present a comparative studyof waveform processing methods to provide insight into selecting an approach for a given combination ofvegetation and terrain characteristics. </p> <p>Methods:</p> <p>We employed two waveform processing methods: </p> <p>(1) direct decomposition, </p> <p>(2) deconvolution and decomposition. </p> <p>In method (2), we utilized two deconvolutionalgorithms - the Richardson-Lucy (RL) algorithm and the Gold algorithm. </p> <p>The comprehensive and quan-titative comparisons were conducted in terms of the number of detected echoes, position accuracy, thebias of the end products (such as digital terrain model (DTM) and canopy height model (CHM)) fromthe corresponding reference data, along with parameter uncertainty for these end products obtainedfrom different methods. </p> <p>Where:</p> <p>This study was conducted at three study sites that include diverse ecological regions, vegetation and elevation gradients. </p> <p>Highlights:</p> <p>(1) Results demonstrate that two deconvolution algorithms are sensitive to the pre-processing steps of input data. The deconvolution and decomposition methodis more capable of detecting hidden echoes with a lower false echo detection rate, especially for theGold algorithm. </p> <p>(2) Compared to the reference data, all approaches generate satisfactory accuracy assess-ment results with small mean spatial difference (&lt;1.22 m for DTMs, &lt;0.77 m for CHMs) and root meansquare error (RMSE) (&lt;1.26 m for DTMs, &lt;1.93 m for CHMs). More specifically, the Gold algorithm is supe-rior to others with smaller root mean square error (RMSE) (&lt;1.01 m), while the direct decompositionapproach works better in terms of the percentage of spatial difference within 0.5 and 1 m.</p> <p>(3) The parameteruncertainty analysis demonstrates that the Gold algorithm outperforms other approaches in dense veg-etation areas, with the smallest RMSE, and the RL algorithm performs better in sparse vegetation areas interms of RMSE. </p> <p>(4) Additionally, the high level of uncertainty occurs more on areas with high slope and highvegetation. </p> <p>This study provides an alternative and innovative approach for waveform processing that willbenefit high fidelity processing of waveform LiDAR data to characterize vegetation structures. </p> <p>{{&lt; youtube id=\"A4MWxAkolO4\" t=\"80\" width=\"300px\" &gt;}}</p>","tags":["algorithm development","Waveform LiDAR","Deconvolution","Gold","Richardson-Lucy (RL)","Decomposition","Parameter uncertainty"]},{"location":"publications/ICESAT-2_BIOMASS/","title":"Estimating aboveground biomass and forest canopy cover with simulated ICESat-2 data","text":"<p>Introduction:</p> <p>The Ice, Cloud and Land Elevation Satellite-2 (ICESat-2) launched on September 15th, 2018 and this mission offers an extraordinary opportunity to contribute to an assessment of forest resources at multiple spatial scales. This study served to develop a methodology for utilizing ICESat-2 data over vegetated areas.</p> <p>Goal: The specific objectives were to:  (1) derive a simulated ICESat-2 photon-counting lidar (PCL) vegetation product using airborne lidar data, </p> <p>(2) examine the use of simulated PCL metrics for modelling aboveground biomass (AGB) along ICESat-2 profiles using a simulated ICESat-2 PCL vegetation product and reference AGB estimated from airborne lidar data, and</p> <p>(3) estimate forest canopy cover using simulated PCL canopy product data and airborne lidarderived canopy cover.</p> <p>Where and scenarios: Using existing airborne lidar data for Sam Houston National Forest (SHNF) in Texas and known ICESat-2 track locations, PCL simulations were carried out. Three scenarios were analyzed in this study;</p> <p>1) simulated data without the addition of noise,</p> <p>2) processed simulated data for daytime, and </p> <p>3) nighttime scenarios.</p> <p>Highlights:</p> <p>product Segments measuring 100m along the proposed ICESat-2 tracks were used to extract simulated PCL metrics from each of the three data scenarios and spatially coincident, reference airborne lidar-estimated AGB and airborne lidar canopy cover estimates. </p> <p>Linear regression models were then developed with a subset of the simulated PCL segments to estimate AGB and canopy cover and their performance assessed using separate testing sets. </p> <p>Three scenarios: AGB model testing with the simulated dataset without noise, nighttime and daytime scenarios resulted in R2 values of 0.79, 0.79 and 0.63 respectively, with root mean square error (RMSE) values of 19.16 Mg/ha, 19.23 Mg/ha, and 25.35 Mg/ha. Predictive models for canopy cover (4.6 m) achieved R2 values of 0.93, 0.75 and 0.63 and RMSE values of 6.36%, 12.33% and 15.01% for the simulated dataset without noise, nighttime and daytime scenarios respectively. </p> <p>Findings from this study suggest the potential of ICESat-2 for estimating AGB, given the photon detection rate and background noise anticipated by ICESat-2 under temperate forest settings, and especially in the data format provided by standard ICESat-2 data vegetation products.</p>","tags":["ICESat-2","AGB","Photon counting lidar","ATL08","Canopy cover","Canopy height","Carbon"]},{"location":"publications/ICESAT-2_BIOMASS_another/","title":"Mapping forest aboveground biomass with a simulated ICESat-2 vegetation canopy product and Landsat data","text":"<p>Introduction: The assessment of forest aboveground biomass (AGB) can contribute to reducing uncertainties associated with the amount and distribution of terrestrial carbon. The Ice, Cloud and land Elevation Satellite-2 (ICESat-2) was launched on September 15th, 2018 and will provide data which will offer the possibility of assessing AGB and forest carbon at multiple spatial scales.</p> <p>Goal: The primary goal of this study was to develop an approach for utilizing data similar to ICESat-2's land-vegetation along track product (ATL08) to generate wall-to-wall AGB maps.</p> <p>How: Utilizing simulated daytime and nighttime ICESat-2 data from planned ICESat-2 tracks over vegetation conditions in south-east Texas, we investigated the integration of Landsat data and derived products for AGB model and map production. Linear regression models were first used to relate simulated photon-counting lidar (PCL) metrics for 100 m segments along ICESat-2 tracks to reference airborne lidar-estimated AGB over Sam Houston National Forest (SHNF) in south-east Texas. Random Forest (RF) was then used to create AGB maps from predicted AGB estimates and explanatory data consisting of spectral metrics derived from Landsat TM imagery and land cover and canopy cover data from the National Land Cover Database (NLCD).</p> <p>Highlights:</p> <p>Using RF, AGB and AGB uncertainty maps produced at 30 m spatial resolution represented three data scenarios; </p> <p>(1) simulated ICESat-2 PCL vegetation product without the impact of noise (no noise scenario), </p> <p>(2) simulated ICESat-2 PCL vegetation product from data with noise levels associated with daytime operation of ICESat-2 (daytime scenario), and </p> <p>(3) simulated ICESat-2 PCL vegetation product from data with noise levels associated with nighttime operation of ICESat-2 (nighttime scenario).</p> <p>The RF models exhibited moderate accuracies (0.42 to 0.51) with RMSE values between 19 Mg/ha to 20 Mg/ha with a separate test set. The adoption of a combinatory approach of simulated ICESat-2 and Landsat data could be implemented at larger spatial scales and in doing so, ancillary data such as climatic and topographic variables may be examined for improving AGB predictions.</p>","tags":["ICESat-2","AGB mapping","Photon counting lidar","ATL08","Canopy cover","Canopy height","LandSat"]},{"location":"publications/Terrestrail_lidar_scan/","title":"Detecting and Quantifying Standing Dead Tree Structural Loss with Reconstructed Tree Models Using Voxelized Terrestrial Lidar Data","text":"<p>Introduction:</p> <p>The structural loss rates of standing dead trees (SDTs) affect a variety of processes of interest to ecologists and foresters, yet the decomposition of SDTs has been traditionally characterized by qualitative decay classes, reductions in wood density as decay progresses, and sampling schemes focused on estimating snag longevity.</p> <p>By establishing a methodology to accurately and efficiently quantify SDT structural loss over time, these estimated structural loss rates would improve the performance of a variety of models and potentially provide new insight as to the manner in which SDTs undergo degradation in various conditions.</p> <p>Goal: The specific objective of this study were: </p> <p>1) utilize the TreeVolX algorithm to estimate the volume of 29 SDTs scanned with terrestrial lidar; </p> <p>2) develop a novel, voxel-based change detection algorithm capable of providing automated structural loss estimates with multitemporal terrestrial lidar observations; and </p> <p>3) estimate and characterize the structural loss rates of Pinus taeda and Quercus stellata in southeastern Texas.</p> <p>Highlights:</p> <p>A voxel-based change detection methodology was developed to accurately detect and quantify structural losses and incorporated several methods to mitigate the challenges presented by shifting tree and branch positions as SDT decay progresses. </p> <p>The volume and structural loss of 29 SDTs, composed of Pinus taeda and Quercus stellata, were successfully estimated using multitemporal terrestrial lidar observations over elapsed times ranging from 71 to 753 days.</p> <p>Pine and oak structural loss rates were characterized by estimating the amount of volumetric loss occurring in 20 equal-interval height bins of each SDT.  Results showed that large pine snags exhibited more rapid structural loss in comparison to medium-sized oak snags in southeastern Texas.</p>","tags":["Biomass","Change detection","Reconstructed tree model (RTM)","Snag","Standing dead tree (SDT)","Structural loss","Terrestrial lidar","Volume, Voxel"]},{"location":"publications/bayesian_decomposition/","title":"Bayesian decomposition of full waveform LiDAR data with uncertainty analysis","text":"<p>Introduction:</p> <p>A thorough understanding of full waveform (FW) LiDAR data processing and associated uncertainty is critical to vegetation applications such as retrieving forest structure variables and estimating forest biomass.</p> <p>Goal:</p> <p>To qunaitfy the uncertaitny of the estimation and gain a deep understanding of waveform lidar processing. </p> <p>Methods:</p> <p>This paper applies the Bayesian non-linear modeling concept to process small-footprint FW LiDAR data (the Bayesian decomposition) collected at a study site of the National Ecological Observatory Network (NEON) to investigate its potential for waveform decomposition and uncertainty estimation.</p> <p>Specifically, several possible models suitable for fitting waveforms were assessed within the Bayesian framework, and the Gaussian model was selected to perform the Bayesian decomposition. Subsequently, we conducted performance evaluation and uncertainty analysis at the parameter, derived point cloud and surface model levels.</p> <p>Highlights:</p> <p>(1) which model to use?</p> <p>Results of the model reasonableness show that the Gaussian model is superior to alternative models with respect to uncertainty, physical meaning and processing efficiency. </p> <p>(2) How the Bayesiand decomposition perform?</p> <p>After converting waveforms to discrete points, the model comparisons demonstrate that the Bayesian decomposition can be utilized for FW LiDAR data processing, and its results are comparable to the direct decomposition (DD), Gold and RL (Richardson-Lucy) approaches in terms of the root mean squared error (RMSE &lt; 0.93 m) of the point distances between the waveform-based point cloud and the reference point cloud. </p> <p>(3) which part of vegetation have evident advantage for waveform lidar?</p> <p>Additionally, more points can be extracted from FW LiDAR data with these methods than discrete-return LiDAR data, especially at the mid-story of vegetation based on the results of height bins, percentile heights and canopy LiDAR density at the individual tree level. </p> <p>(4) Uncertainy analysis</p> <p>Moreover, uncertainty estimates from the Bayesian method enhance the credibility of decomposition results in a probabilistic sense to capture the true error of estimates and trace the uncertainty propagation along the processing steps. For example, results of the surface model yield larger RMSE values (1.38 m vs. 0.65 m) with a wider credible interval than quantile point clouds with a more compact distribution.</p> <p>advantages of Bayesian</p> <p>In contrast to commonly used deterministic approaches, the Bayesian decomposition method can produce an ensemble of reasonable parameter estimates with probability through Markov Chain Monte Carlo (MCMC) sampling from the posterior distribution of model parameters. </p> <p>These parameter estimates and corresponding derived products can be queried to provide meaningful interpretation of results and associated uncertainty. Both the flat priors and empirical priors can achieve good performance of the decomposition while the empirical priors tend to significantly speed up the model convergence</p> <p>This study provides an alternative and innovative approach for waveform processing that willbenefit high fidelity processing of waveform LiDAR data to characterize vegetation structures. </p>","tags":["algorithm development","Waveform LiDAR","Bayesian inference","Tree canopy height","Model reasonableness","Decomposition","Uncertainty"]},{"location":"publications/deep_learning_count_panicles/","title":"A Deep Learning Semantic Segmentation-Based Approach for Field-Level Sorghum Panicle Counting","text":"<p>Introduction:</p> <p>Small unmanned aerial systems (UAS) have emerged as high-throughput platforms for the collection of high-resolution image data over large crop fields to support precision agriculture and plant breeding research. At the same time, the improved efficiency in image capture is leading to massive datasets, which pose analysis challenges in providing needed phenotypic data.</p> <p>To complement these high-throughput platforms, there is an increasing need in crop improvement to develop robust image analysis methods to analyze large amount of image data. Analysis approaches based on deep learning models are currently the most promising and show unparalleled performance in analyzing large image datasets.</p> <p>Goal:</p> <p>This study developed and applied an image analysis approach based on a SegNet deep learning semantic segmentation model to estimate sorghum panicles counts, which are critical phenotypic data in sorghum crop improvement, from UAS images over selected sorghum experimental plots.</p> <p>Methods:</p> <p>The SegNet model was trained to semantically segment UAS images into sorghum panicles, foliage and the exposed ground using 462, 250 * 250 labeled images, which was then applied to field orthomosaic to generate a field-level semantic segmentation.</p> <p>Highlights:</p> <p>(1) post processing</p> <p>Individual panicle locations were obtained after post-processing the segmentation output to remove small objects and split merged panicles. A comparison between model panicle count estimates and manually digitized panicle locations in 60 randomly selected plots showed an overall detection accuracy of 94%. </p> <p>(2) plot level</p> <p>A per-plot panicle count comparison also showed high agreement between estimated and reference panicle counts (Spearman correlation r = 0.88, mean bias = 0.65).</p> <p>(3) where is the error coming from?</p> <p>Misclassifications of panicles during the semantic segmentation step and mosaicking errors in the field orthomosaic contributed mainly to panicle detection errors.</p> <p>Overall, the approach based on deep learning semantic segmentation showed good promise and with a larger labeled dataset and extensive hyper-parameter tuning, should provide even more robust and effective characterization of sorghum panicle counts. </p>","tags":["deep learning","semantic segmentation","sorghum panicle","plant phenotyping","unmanned aerial systems (UAS)","plant breeding","automation","counting"]},{"location":"publications/hyper_point_cloud/","title":"From LiDARWaveforms to Hyper Point Clouds: A Novel Data Product to Characterize Vegetation Structure","text":"<p>Introduction:</p> <p>Full waveform (FW) LiDAR holds great potential for retrieving vegetation structure parameters at a high level of detail, but this prospect is constrained by practical factors such as the lack of available handy processing tools and the technical intricacy of waveform processing.</p> <p>Methods:</p> <p>This study introduces a new product named the Hyper Point Cloud (HPC), derived from FW LiDAR data, and explores its potential applications, such as tree crown delineation using the HPC-based intensity and percentile height (PH) surfaces, which shows promise as a solution to the constraints of using FW LiDAR data.</p> <p>Highlight:</p> <p>(1) why named hyper point cloud? </p> <p>The results of the HPC present a new direction for handling FW LiDAR data and offer prospects for studying the mid-story and understory of vegetation with high point density (~182 points/m2).</p> <p>(2) what HPC can do?</p> <p>The intensity-derived digital surface model (DSM) generated from the HPC shows that the ground region has higher maximum intensity (MAXI) and mean intensity (MI) than the vegetation region, while having lower total intensity (TI) and number of intensities (NI) at a given grid cell.</p> <p>(3) why contour of intensity can work for tree segmentation?</p> <p>Our analysis of intensity distribution contours at the individual tree level exhibit similar patterns, indicating that the MAXI and MI decrease from the tree crown center to the tree boundary, while a rising trend is observed for TI and NI. </p> <p>These intensity variable contours provide a theoretical justification for using HPC-based intensity surfaces to segment tree crowns and exploit their potential for extracting tree attributes. The HPC-based intensity surfaces and the HPC-based PH Canopy Height Models (CHM) demonstrate promising tree segmentation results comparable to the LiDAR-derived CHM for estimating tree attributes such as tree locations, crown widths and tree heights.</p> <p>We envision that products such as the HPC and the HPC-based intensity and height surfaces introduced in this study can open new perspectives for the use of FW LiDAR data and alleviate the technical barrier of exploring FW LiDAR data for detailed vegetation structure characterization.</p>","tags":["hyper point cloud (HPC)","HPC-based intensity surface","gridding","tree segmentation","vegetation structure"]},{"location":"publications/photon_couting_algorithms/","title":"Photon counting LiDAR: An adaptive ground and canopy height retrieval algorithm for ICESat-2 data","text":"<p>Introduction:</p> <p>The upcoming Ice, Cloud and Land Elevation Satellite-2 (ICESat-2) mission will offer prospects for mapping and monitoring biomass and carbon of terrestrial ecosystems over large areas using photon counting LiDAR data</p> <p>Goal:</p> <p>We aim to develop a methodology to derive terrain elevation and vegetation canopy height from testbed sensor data and further pre-validate the capacity of the mission to meet its science objectives for the ecosystem community.</p> <p>Methods:</p> <p>We investigated a novel methodological framework with two essential steps for characterizing terrain and canopy height using Multiple Altimeter Beam Experimental LiDAR (MABEL) data and simulated ICESat-2 data with various vegetation conditions. Our algorithm first implements a multi-level noise filtering approach to minimize noise photons and subsequently classifies the remaining photons into ground and top of canopy using an overlapping moving window method and cubic spline interpolation.</p> <p>Highlight:</p> <p>(1) Noise filtering: </p> <p>Results of noise filtering show that the design of the multi-level filtering process is effective to identify background noise and preserve signal photons in the raw data. </p> <p>(2) Day time vs. night time</p> <p>Moreover, calibration results using MABEL and simulated ICESat-2 data share similar trends with the retrieved terrain being more accurate than the retrieved canopy height, and the nighttime results being better than corresponding daytime results.</p> <p>(3) Simulated ICESat-2 vs. MABEL</p> <p>Compared to the results of simulated ICESat-2 data, MABEL data achieve lower accuracy for ground and canopy heights in terms of root mean square error (RMSE), which may partly result from the inconsistency between MABEL and reference data. Specifically, simulated ICESat-2 data using 115 various nighttime and daytime scenarios, yield average RMSE values of 1.83m and 2.80m for estimated ground elevation, and 2.70m and 3.59m for estimated canopy height.</p> <p>(4) Percentile height validation Additionally, the accuracy assessment of percentile heights of simulated ICESat-2 data further substantiates the robustness of the methodology from different perspectives. </p> <p>The methodology developed in this study illustrates plausible ways of processing the data that are structurally similar to expected ICESat-2 data and holds the potential to be a benchmark for further method adjustment once genuine ICESat-2 are available.</p>","tags":["ICESat-2","ATL03, ATL08","Photon classification","Photon counting LiDAR","ATLAS","MABEL","Canopy height","Terrain elevation"]},{"location":"publications/waveformlidar_R_package/","title":"waveformlidar: An R Package for Waveform LiDAR Processing and Analysis","text":"<p>Introduction:</p> <p>A wealth of FullWaveform (FW) LiDAR (Light Detection and Ranging) data are available to the public from different sources, which is poised to boost extensive applications of FW LiDAR data. However, we lack a handy and open source tool that can be used by potential users for processing and analyzing FW LiDAR data. </p> <p>Goal: To this end, we introduce waveformlidar, an R package dedicated to FW LiDAR processing, analysis and visualization as a solution to the constraint.</p> <p>What this package provide:</p> <p>(1) This package provides several commonly used waveform processing methods such as Gaussian, Adaptive Gaussian and Weibull decompositions and deconvolution approaches (Gold and Richard-Lucy (RL)) with users' customized settings.</p> <p>(2) we also developed functions to derive commonly used waveform metrics for characterizing vegetation structure. </p> <p>(3) Moreover, a new way to directly visualize FW LiDAR data is developed by converting waveforms into points to form the Hyper Point Cloud (HPC), which can be easily adopted and subsequently analyzed with existing discrete-return LiDAR processing tools such as LAStools and FUSION.</p> <p>(4) Basic explorations of the HPC such as 3D voxelization of the HPC and conversion from original waveforms to composite waveforms are also available in this package.</p> <p>All of these functions are developed based on small-footprint FW LiDAR data but they can be easily transplanted to the large footprint FW LiDAR data such as Geoscience Laser Altimeter System (GLAS) and Global Ecosystem Dynamics Investigation (GEDI) data analysis. </p> <p>It is anticipated that these functions will facilitate the widespread use of FW LiDAR and be beneficial for better estimating biomass and characterizing vegetation structure at various scales.</p>","tags":["waveform decomposition","hyper point cloud","deconvolution","Waveform signatures","waveform gridding & voxel","composite waveform"]}]}